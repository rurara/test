{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b11b80-4b66-41e7-973e-2e56fa12281f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#https://zorba-blog.tistory.com/22\n",
    "#$ pip install torchmetrics\n",
    "torch.cuda.is_available() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27162497-170d-4250-9088-4e0971656dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ratsnlp.nlpbook.ner import NERTrainArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c053b60-4a94-4d74-bef1-7d9c8d3e590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = NERTrainArguments(\n",
    "    pretrained_model_name='beomi/kcbert-base',\n",
    "    downstream_corpus_name='ner',\n",
    "    downstream_model_dir='checkpoint-ner',\n",
    "    batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    max_seq_length=64,\n",
    "    epochs=3,\n",
    "    tpu_cores=8,\n",
    "    seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423ea111-f346-4386-8a77-8a93f5650a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 7\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6687c7-c5f1-4b3c-b983-c07df0d0640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='named-entity-recognition', downstream_corpus_name='ner', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='checkpoint-ner', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=4, cpu_workers=4, fp16=False, tpu_cores=8)\n"
     ]
    }
   ],
   "source": [
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f806846-6edc-4c54-b281-92ed3d6251cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 17.9M/17.9M [00:01<00:00, 11.9MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.13M/1.13M [00:00<00:00, 3.39MB/s]\n"
     ]
    }
   ],
   "source": [
    "nlpbook.download_downstream_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0529370-b7cc-4a62-a279-790edcff1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63108da5-96c0-43f8-bdbe-0f5ad38148a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Creating features from dataset file at /content/Korpora\\ner\n",
      "INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora\\ner\\train.txt\n",
      "INFO:ratsnlp:processing NER tag dictionary...\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n",
      "INFO:ratsnlp:target: 이어 옆으로 움직여 <김일성:PER>의 오른쪽에서 <한 차례:NOH>씩 <두 번:NOH> 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n",
      "\n",
      "INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n",
      "INFO:ratsnlp:target: <제철과일리코타치즈샐러드:POH>는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n",
      "\n",
      "INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n",
      "INFO:ratsnlp:target: <정:PER>씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n",
      "\n",
      "INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다.\n",
      "INFO:ratsnlp:target: ―<효진:PER> 역의 <김환희:PER>(<14:NOH>)가 특히 인상적이었다.\n",
      "\n",
      "INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n",
      "INFO:ratsnlp:target: 전문가들은 <미국:ORG> 증시의 상승세가 유지되고 ‘<트럼프노믹스:POH>’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n",
      "\n",
      "INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n",
      "INFO:ratsnlp:Saving features into cached file /content/Korpora\\ner\\cached_train_BertTokenizer_64_ner_named-entity-recognition [took 6.888 s]\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.ner import NERCorpus, NERDataset\n",
    "corpus = NERCorpus(args)\n",
    "train_dataset = NERDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46752bbc-79a6-42e8-8501-7db22fb17e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f2dc73-a31f-487a-8f36-dbf382cafaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Loading features from cached file /content/Korpora\\ner\\cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.085 s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "val_dataset = NERDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode='val')\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f99d383-6ed2-446a-9247-9dd55316fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForTokenClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=corpus.num_labels)\n",
    "model=BertForTokenClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1952899-1b0d-4247-8628-34b9eb9800ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.ner import NERTask\n",
    "task = NERTask(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff6b09c-b8c7-4286-85f5-25abc4f0300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:464: UserWarning: more than one device specific flag has been set\n",
      "  rank_zero_warn(\"more than one device specific flag has been set\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = nlpbook.get_trainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c91a0-95cb-4a43-aaf6-7108e57b4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: D:\\developer\\test\\test\\bert_gpt\\checkpoint-ner\\lightning_logs\n",
      "C:\\ProgramData\\Anaconda3\\envs\\39\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:608: UserWarning: Checkpoint directory D:\\developer\\test\\test\\bert_gpt\\checkpoint-ner exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\39\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | BertForTokenClassification | 108 M \n",
      "-----------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.389   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fb1cddc721445e8250becd52f64a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
