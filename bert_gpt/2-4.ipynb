{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d3a70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('bbpe')\n",
    "tokenizer_gpt.pad_token = '[PAD]'\n",
    "\n",
    "#vacab.json을 config로 바꿔야함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eddd1fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
       " ['íĿł',\n",
       "  '...',\n",
       "  'íı¬ìĬ¤íĦ°',\n",
       "  'ë³´ê³ł',\n",
       "  'Ġì´ĪëĶ©',\n",
       "  'ìĺģíĻĶ',\n",
       "  'ì¤Ħ',\n",
       "  '....',\n",
       "  'ìĺ¤ë²Ħ',\n",
       "  'ìĹ°ê¸°',\n",
       "  'ì¡°ì°¨',\n",
       "  'Ġê°Ģë³į',\n",
       "  'ì§Ģ',\n",
       "  'ĠìķĬ',\n",
       "  'êµ¬ëĤĺ'],\n",
       " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\n",
    "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
    "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
    "    \"별루 였다..\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427a973f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[335, 2339, 264, 582, 4056, 465, 3809, 0, 0, 0, 0, 0], [3694, 337, 2877, 759, 2884, 357, 807, 423, 9876, 876, 2961, 7293], [4958, 452, 3654, 264, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs = tokenizer_gpt(\n",
    "    sentences,\n",
    "    padding='max_length',\n",
    "    max_length=12,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c0da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file wordpiece\\config.json not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='wordpiece', vocab_size=10000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('wordpiece', do_lower_case=False)\n",
    "tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c742f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_bert = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
