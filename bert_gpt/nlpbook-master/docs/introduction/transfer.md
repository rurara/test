---
layout: default
title: Transfer Learning
parent: Introduction
nav_order: 2
---

# 트랜스퍼 러닝 (Transfer Learning)
{: .no_toc }

이 책에서 소개하는 자연어 처리 모델 학습 방법은 트랜스퍼 러닝(Transfer Learning)입니다. 이 장에서는 프리트레인(pretrain), 파인튜닝(finetuning) 등 트랜스퍼 러닝과 관련된 개념을 설명합니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---

## 트랜스퍼 러닝

**트랜스퍼 러닝(transfer learning)**이란 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법을 가리킵니다. 비유하자면 사람이 새로운 지식을 배울 때 그가 평생 쌓아왔던 지식을 요긴하게 다시 써먹는 것과 같습니다.

그림1처럼 `Task2`를 수행하는 모델을 만든다고 가정해 보면 트랜스퍼 러닝이 도움이 될 수 있습니다. 모델이 `Task2`를 배울 때 `Task1`을 수행해 봤던 경험을 재활용하기 때문입니다. 

## **그림1** 트랜스퍼 러닝 (Transfer Learning)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/cNvHXxj.png" width="400px" title="source: imgur.com" />

트랜스퍼 러닝을 적용하면 기존\*보다 모델의 학습 속도가 빨라지고 새로운 태스크(`Task2`)를 더 잘 수행하는 경향이 있습니다. 이 때문에 트랜스퍼 러닝은 최근 널리 쓰이고 있습니다. BERT(Bidirectional Encoder Representations from Transformers)나 GPT(Generative Pre-trained Transformer) 등이 바로 이 기법을 쓰고 있습니다.

\* 트랜스퍼 러닝 적용 이전의 기존 모델은 태스크를 처음부터 학습했습니다. 사람의 학습에 비유하면 아무런 사전 지식 없이 새로운 지식을 배우는 것과 같습니다.
{: .fs-4 .ls-1 .code-example }

그림2의 `Task1`은 **업스트림(upstream) 태스크**라고 부르고 `Task2`는 이와 대비된 개념으로 **다운스트림(downstream) 태스크**라고 부릅니다. `Task1`은 다음 단어 맞히기, 빈칸 채우기 등 대규모 말뭉치의 문맥을 이해하는 과제이며, `Task2`는 문서 분류, 개체명 인식 등 우리가 풀고 싶은 자연어 처리의 구체적 문제들입니다.

업스트림 태스크를 학습하는 과정을 **프리트레인(pretrain)**이라고 합니다. 다운스트림 태스크를 본격적으로 수행하기에 앞서(pre) 학습(train)한다는 의미에서 이런 용어가 붙은 것으로 생각합니다.


---


## 업스트림 태스크

트랜스퍼 러닝이 주목받게 된 것은 업스트림 태스크와 프리트레인 덕분입니다. 자연어의 풍부한 **문맥(context)**을 모델에 내재화하고 이 모델을 다양한 다운스트림 태스크에 적용해 그 성능을 대폭 끌어올리게 된 것이죠.

대표적인 업스트림 태스크 가운데 하나가 **다음 단어 맞추기**입니다. GPT 계열 모델이 바로 이 태스크로 프리트레인을 수행합니다. 그림2처럼 `티끌 모아`라는 문맥이 주어졌고 학습 데이터 말뭉치에 `티끌 모아 태산`이라는 구(phrase)가 많다고 하면 모델은 이를 바탕으로 다음 단어를 `태산`으로 분류하도록 학습됩니다.

## **그림2** 다음 단어 맞추기
{: .no_toc .text-delta }
<img src="https://i.imgur.com/HxRAUDp.png" width="240px" title="source: imgur.com" />

모델이 대규모 말뭉치를 가지고 이런 과정을 반복 수행하게 되면 이전 문맥을 고려했을 때 어떤 단어가 그다음에 오는 것이 자연스러운지 알 수 있게 됩니다. 다시 말해 해당 언어의 풍부한 문맥을 이해할 수 있게 된다는 것이죠. 이처럼 '다음 단어 맞추기'로 업스트림 태스크를 수행한 모델을 **언어 모델(Language Model)**이라고 합니다.

언어 모델을 학습하는 것은 [이전 절](https://ratsgo.github.io/nlpbook/docs/introduction/deepnlp)에서 언급한 감성 분석 모델의 학습 과정과 별반 다르지 않습니다. 감성 분석 예시에서는 분류해야 할 범주의 수가 3개(긍정, 중립, 부정)뿐이었지만, 언어모델에서는 학습 대상 언어의 어휘 수(보통 수만 개 이상)만큼 늘어납니다. 예를 들어 `티끌 모아` 다음 단어의 정답이 `태산`이라면 `태산`이라는 단어에 해당하는 확률은 높이고 나머지 단어에 관계된 확률은 낮추는 방향으로 모델 전체를 업데이트합니다. 그림3과 같습니다.

## **그림3** 언어 모델 학습
{: .no_toc .text-delta }
<img src="https://i.imgur.com/TlR365S.jpg" width="300px" title="source: imgur.com" />


또 다른 업스트림 태스크로는 **빈칸 채우기**가 있습니다. BERT 계열 모델이 바로 이 태스크로 프리트레인을 수행합니다. 그림4처럼 문장에서 빈칸으로 만들고 해당 빈칸에 들어갈 단어가 무엇일지 맞히는 과정에서 모델이 학습됩니다.

## **그림4** 빈칸 채우기
{: .no_toc .text-delta }
<img src="https://i.imgur.com/kfkf6bw.png" width="220px" title="source: imgur.com" />

모델이 많은 양의 데이터를 가지고 빈칸 채우기를 반복 학습하게 되면 앞뒤 문맥을 보고 빈칸에 적합한 단어를 알 수 있습니다. 이 태스크를 수행한 모델 역시 언어 모델과 마찬가지로 해당 언어의 풍부한 문맥을 내재화할 수 있습니다. 이처럼 '빈칸 채우기'로 업스트림 태스크를 수행한 모델을 **마스크 언어 모델(Masked Language Model)**이라고 합니다.

마스크 언어모델 학습 역시 언어 모델과 비슷합니다. 그림4에서 빈칸 정답이 `모아`라면 `모아`라는 단어에 해당하는 확률은 높이고 나머지 단어에 관계된 확률은 낮추는 방향으로 모델 전체를 업데이트합니다. 그림5와 같습니다.

## **그림5** 마스크 언어 모델 학습
{: .no_toc .text-delta }
<img src="https://i.imgur.com/YHa2Vva.jpg" width="300px" title="source: imgur.com" />

[이전 절](https://ratsgo.github.io/nlpbook/docs/introduction/deepnlp)에서 살펴본 감성 분석 모델 학습 예시에서 학습 데이터는 사람이 일일이 정답(레이블)을 만들어 줘야 했습니다. 이처럼 사람이 만든 정답 데이터로 모델을 학습하는 방법을 **지도 학습(supervised learning)**이라고 합니다. 이 방식은 데이터를 만드는 데 비용이 많이 들뿐만 아니라 실수로 잘못된 레이블을 줄 수도 있습니다.

이에 반해 다음 단어 맞히기, 빈칸 채우기 같은 업스트림 태스크는 강력한 힘을 지닙니다. 뉴스, 웹 문서, 백과사전 등 글만 있으면 수작업 없이도 다량의 학습 데이터를 아주 싼값에 만들어 낼 수 있습니다. 덕분에 업스트림 태스크를 수행한 모델은 성능이 기존보다 월등히 좋아졌습니다. 이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 **자기 지도 학습(self-supervised learning)**이라고 합니다.


---

## 다운스트림 태스크

우리가 모델을 업스트림 태스크로 프리트레인한 근본 이유는 다운스트림 태스크를 잘 하기 위해서입니다. 앞에서 설명했듯이 다운스트림 태스크는 우리가 풀어야할 자연어 처리의 구체적 과제들입니다. 보통 다운스트림 태스크는 프리트레인을 마친 모델을 구조 변경 없이 그대로 사용하거나 여기에 태스크 모듈을 덧붙인 형태로 수행합니다.

이 책에서 소개하는 다운스트림 태스크의 그 본질은 **분류(classification)**입니다. 다시 말해 자연어를 입력 받아 해당 입력이 어떤 범주에 해당하는지 확률 형태로 반환합니다. 문장 생성을 제외한 대부분의 과제에서는 프리트레인을 마친 마스크 언어 모델(BERT 계열)을 사용합니다. 

이 책에서 설명하는 다운스트림 태스크의 학습 방식은 모두 **파인튜닝(fine-tuning)**입니다. 파인튜닝은 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트하는 기법입니다. 예를 들어 문서 분류를 수행할 경우 프리트레인을 마친 BERT 모델 전체를 문서 분류 데이터로 업데이트합니다. 마찬가지로 개체명 인식을 수행한다면 BERT 모델 전체를 해당 데이터로 업데이트합니다.

그러면 각 다운스트림 태스크를 차례대로 살펴보겠습니다.

### 문서 분류

문서 분류 모델은 자연어(문서나 문장)를 입력받아 해당 입력이 어떤 범주(긍정, 중립, 부정 따위)에 속하는지 그 확률값을 반환합니다.

## **그림6** 문서 분류
{: .no_toc .text-delta }
<img src="https://i.imgur.com/HYFh7Eg.png" width="350px" title="source: imgur.com" />

구체적으로는 프리트레인을 마친 마스크 언어모델(그림6에서 노란색 실선 박스) 위에 작은 모듈(초록색 실선 박스)을 하나 더 쌓아 문서 전체의 범주를 분류합니다. 자세한 내용은 [Document Classification](https://ratsgo.github.io/nlpbook/docs/classification)을 참고하세요.

한편 그림6 이후에 나오는 `CLS`, `SEP`는 각각 문장의 시작과 끝에 붙이는 특수한 토큰(token)입니다. 자세한 내용은 [Preprocess](https://ratsgo.github.io/nlpbook/docs/preprocess)에서 다룹니다.


### 자연어 추론

자연어 추론 모델은 문장 2개를 입력받아 두 문장 사이의 관계가 참(entailment), 거짓(contradiction), 중립(neutral) 등 어떤 범주인지 그 확률값을 반환합니다.

## **그림7** 자연어 추론
{: .no_toc .text-delta }
<img src="https://i.imgur.com/7Y7RERd.png" width="450px" title="source: imgur.com" />

구체적으로는 프리트레인을 마친 마스크 언어모델(그림7에서 노란색 실선 박스) 위에 작은 모듈(초록샌 실선 박스)을 하나 더 쌓아 두 문장의 관계 범주를 분류합니다. 자세한 내용은 [Pair Classification](http://ratsgo.github.io/nlpbook/docs/pair_cls)을 참고하세요.


### 개체명 인식

개체명 인식 모델은 자연어(문서나 문장)를 입력받아 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 그 확률값을 반환합니다.

## **그림8** 개체명 인식
{: .no_toc .text-delta }
<img src="https://i.imgur.com/dt6diUT.png" width="350px" title="source: imgur.com" />

구체적으로는 프리트레인을 마친 마스크 언어 모델(그림8에서 노란색 실선 박스) 위에 단어별로 작은 모듈(초록색 실선 박스)을 쌓아 단어 각각의 개체명 범주를 분류합니다. 자세한 내용은 [Named Entity Recognition](https://ratsgo.github.io/nlpbook/docs/ner)을 참고하세요.


### 질의응답

질의응답 모델은 자연어(질문+지문)를 입력받아 각 단어가 정답의 시작일 확률값과 끝일 확률값을 반환합니다.

## **그림8** 질의응답
{: .no_toc .text-delta }
<img src="https://i.imgur.com/hRXRNcm.png" width="500px" title="source: imgur.com" />

구체적으로는 프리트레인을 마친 마스크 언어모델(그림8에서 노란색 실선 박스) 위에 단어별로 작은 모듈을 각각 더 쌓아 전체 단어 가운데 어떤 단어가 시작(초록색 실선 박스)인지 끝(붉은색 실선 박스)인지 분류합니다. 자세한 내용은 [Question Answering](https://ratsgo.github.io/nlpbook/docs/qa)을 참고하세요.


### 문장 생성

문장 생성 모델은 GPT 계열 언어 모델이 널리 쓰입니다. 문장 생성 모델은 자연어(문장)를 입력받아 어휘 전체에 대한 확률값을 반환합니다. 이 확률값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수입니다. 

## **그림9** 문장 생성
{: .no_toc .text-delta }
<img src="https://i.imgur.com/DEcerbd.png" width="250px" title="source: imgur.com" />

구체적으로는 프리트레인을 마친 언어모델(그림9에서 보라색 실선 박스)을 구조 변경 없이 그대로 사용해, 문맥에 이어지는 적절한 다음 단어를 분류하는 방식입니다. 자세한 내용은 [Sentence Generation](https://ratsgo.github.io/nlpbook/docs/generation)을 참고하세요.

---


## 트랜스퍼 러닝의 다양한 기법들 

앞서 소개한 문서 분류, 자연어 추론, 질의응답, 문장 생성 모델은 모두 파인튜닝 방식으로 학습합니다. 하지만 다운스트림 태스크를 학습하는 방식은 파인튜닝 말고도 다양합니다. 크게 다음 3가지가 있습니다.

- **파인튜닝(fine-tuning)** : 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 전체를 업데이트합니다.
- **프롬프트 튜닝(prompt tuning)** : 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 일부만 업데이트합니다.
- **인컨텍스트 러닝(in-context learning)** : 다운스트림 태스크 데이터의 일부만 사용합니다. 모델을 업데이트하지 않습니다.

파인튜닝 이외의 방식이 주목받고 있는 이유는 비용과 성능 때문입니다. 최근 언어 모델의 크기가 기하급수로 커지고 있는데요, 파인튜닝 방식으로 모델 전체를 업데이트하려면 많은 비용이 듭니다. 그 뿐만 아니라 프롬프트 튜닝, 인컨텍스트 러닝으로 학습한 모델이 경쟁력 있는 태스크 수행 성능을 보일 때가 많습니다.
인컨텍스트 러닝에는 다음 3가지 방식이 있습니다. 다운스트림 태스크 데이터를 몇 건 참고하느냐의 차이가 있을 뿐 모두 모델을 업데이트하지 않는다는 공통점이 있습니다. 모델을 업데이트하지 않고도 다운스트림 태스크를 바로 수행할 수 있다는 건 꽤나 매력적입니다.

- **제로샷러닝(zero-shot learning)** : 다운스트림 태스크 데이터를 전혀 사용하지 않습니다. 모델이 바로 다운스트림 태스크를 수행합니다.
- **원샷러닝(one-shot learning)** : 다운스트림 태스크 데이터를 1 건만 사용합니다. 모델은 1건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행합니다.
- **퓨샷러닝(few-shot learning)** : 다운스트림 태스크 데이터를 몇 건만 사용합니다. 모델은 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다.

---