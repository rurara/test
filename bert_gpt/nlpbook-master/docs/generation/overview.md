---
layout: default
title: Overview
parent: Sentence Generation
nav_order: 1
---

# 전체적으로 훑어보기
{: .no_toc }

이 장에서는 문서 생성 과제를 실습합니다. 모델 아키텍처, 입/출력 등 전반을 조망합니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---

## 과제 소개


문장 생성(Sentence Generation)이란 말 그대로 문장을 만들어내는 과제입니다. 구체적으로는 이전 단어들, 즉 컨텍스트(context)가 주어졌을 때 다음 단어로 어떤 단어가 오는 게 적절한지 분류하는 것입니다. 문장 생성 과제에서 모델의 입력은 컨텍스트, 출력은 컨텍스트 다음 토큰의 등장 확률이 됩니다. 

예컨대 컨텍스트가 "안녕"일 때 모델의 입출력은 다음과 같습니다. 입력은 컨텍스트를 토큰화한 결과(토큰 시퀀스)이며 출력은 다음 토큰에 대한 확률 분포입니다. 이 확률 분포의 길이는 어휘 집합(vocabulary) 크기와 같습니다. 다시 말해 모델은 전체 어휘 각각에 대해 컨텍스트 토큰 시퀀스 바로 다음에 올 토큰으로 얼마나 그럴듯한지에 관한 수치를 나타내는 역할을 한다고 볼 수 있겠습니다.

- 입력 : 안녕
- 출력
  - 가 : 0.002
  - 가격 : 0.0001
  - ...
  - 아 : 0.0001
  - 안 : 0.0002
  - 안녕 : 0.0003
  - ...
  - 하 : 0.2
  - 하다 : 0.15
  - 하세요 : 0.3
  - ...
  - ! : 0.16
  - ? : 0.18
  - ...

그림1은 컨텍스트가 "안녕"일 때 모델의 출력, 즉 $\text{P}(w\|\text{안녕})$을 나타냅니다. 위의 예시를 그림으로 다시 그린 것이라고 보면 될 것 같습니다. 위의 예시와 그림에서 확인할 수 있듯 모델은 "안녕" 다음 토큰으로 "하세요"가 가장 그럴듯하다고 예측하고 있습니다. 이를 근거로 우리는 "안녕" 다음 토큰으로 "하세요"를 선택할 수 있게 됩니다.


## **그림1** 문서 생성 (1)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/neNUDPT.jpg" width="500px" title="source: imgur.com" />


그림2는 "안녕, 하세요"를 컨텍스트로 해서 모델이 출력한 다음 토큰 확률 분포, 즉 $\text{P}(w\|\text{안녕, 하세요})$를 도식화한 것입니다. 이번에는 모델이 "안녕, 하세요" 다음 토큰으로 "!"를 가장 그럴듯하다고 예측하고 있습니다. 이에 우리는 "안녕, 하세요" 다음 토큰으로 "!"를 선택할 수 있게 됩니다.


## **그림2** 문서 생성 (2)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/HCvQqqJ.jpg" width="500px" title="source: imgur.com" />


문장 생성 방식은 다음과 같습니다. (1) 컨텍스트를 모델에 입력해 다음 토큰 확률, 즉 $\text{P}(w\|\text{context})$을 출력하게 한 뒤 다음 토큰을 선택합니다. (2) 기존 컨텍스트에 (1)에서 선택한 다음 토큰을 이어 붙인 새로운 컨텍스트를 모델에 입력해서 다음 토큰 확률 분포, 즉 $\text{P}(w\|\text{new context})$를 추출하고 그 다음 토큰을 선택합니다. (3) 이를 반복해 다음 토큰을 계속 생성해 나갑니다. 

그림1과 그림2의 예시에서는 모델이 출력한 다음 토큰 확률 분포 가운데 가장 확률값이 높은 단어를 다음 토큰으로 선택했는데요. 다음 토큰을 선택하는 과정은 다양한 변종이 존재합니다. 이와 관련해서는 이 장의 [inference (1)](http://ratsgo.github.io/nlpbook/docs/generation/inference1)과 [inference (2)](http://ratsgo.github.io/nlpbook/docs/generation/inference2) 챕터에서 자세히 살펴보겠습니다. 이렇게 생성한 토큰 시퀀스를 적당한 후처리(post processing)해서 사람이 보기에 좋은 형태로 가공해 주면 우리가 원하는 최종 문장 생성 결과가 됩니다. 

우리 책 문장 생성 튜토리얼에서는 [NSMC(Naver Sentiment Movie Corpus)](https://github.com/e9t/nsmc) 데이터를 활용해, SK텔레콤이 공개한 [KoGPT2](https://github.com/SKT-AI/KoGPT2) 모델을 파인튜닝하는 실습을 진행합니다.


---


## 모델 구조

우리 책에서 사용하는 문장 생성 모델은 **언어 모델(Language Model)**입니다. [3장](http://ratsgo.github.io/nlpbook/docs/language_model/semantics/)에서 이미 살펴보았듯 컨텍스트(이전 단어들)가 주어졌을 때 다음 단어를 맞추는 방식으로 프리트레인(pretrain)을 수행한 모델입니다. 그림3과 같습니다.

## **그림3** 언어모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/2Q3BIkc.png" width="350px" title="source: imgur.com" />

문장 생성 과제는 문서 분류(5장), 문서 쌍 분류(6장), 개체명 인식(7장), 질의 응답(8장) 등 우리가 이미 실습한 기존 과제들과는 다른 특성을 가지고 있습니다. 그 차이를 도식적으로 나타내면 표1과 같습니다. 모델 구조와 프리트레인 태스크와 관련한 자세한 내용은 [3장 BERT와 GPT](http://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/)를 참고하시면 좋을 것 같습니다.

## **표1** 기존 과제와의 차이점
{: .no_toc .text-delta }

|항목|문장 생성|기존 과제|
|---|---|---|
|모델 구조|GPT(트랜스포머의 디코더)|BERT(트랜스포머의 인코더)|
|프리트레인 태스크|다음 단어 맞추기|빈칸 맞추기|
|파인튜닝|다음 단어 맞추기|각 다운스트림 태스크|

문장 생성 과제와 기존 과제의 큰 차이점 가운데 하나는 파인튜닝입니다. 기존 과제는 '빈칸 맞추기'로 프리트레인을 수행하고 NSMC(문서 분류), KLUE-NLI(문서 쌍 분류), KorQuAD(질의 응답) 등 각 세부 분야 다운스트림 태스크 관련 데이터로 파인튜닝합니다. 프리트레인 태스크와 파인튜닝 태스크가 서로 다른 구조입니다. 하지만 문장 생성 과제는 프리트레인과 파인튜닝 태스크가 '다음 단어 맞추기'로 같습니다. 

아울러 기존 과제는 프리트레인 모델을 그대로 파인튜닝에 사용하기 어렵습니다. 위에 태스크 모듈을 붙여야 파인튜닝이 가능합니다. 그도 그럴 것이 프리트레인 태스크와 파인튜닝 태스크가 서로 다르기 때문입니다. 하지만 문장 생성 과제는 프리트레인과 파인튜닝 태스크가 동일하므로 프리트레인 모델의 변경 없이 그대로 파인튜닝을 수행할 수 있습니다.

예컨대 파인튜닝 데이터의 이번 입력 단어 시퀀스가 `이 영화 정말 재미`이고 이번에 `있었어`를 맞춰야 하는 상황이라고 가정해 보겠습니다. 이 경우 이번 시점의 정답인 `있었어`에 해당하는 모델 출력 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트합니다. 이것이 문장 생성 과제의 파인튜닝입니다.

---

