---
layout: default
title: Overview
parent: Named Entity Recognition
nav_order: 1
---

# 전체적으로 훑어보기
{: .no_toc }

이 장에서는 개체명 인식 과제를 실습합니다. 모델 아키텍처, 입/출력 등 전반을 조망합니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---


## 과제 소개


개체명 인식(Named Entity Recognition)이란 문장을 토큰화한 뒤 토큰 각각에 인명, 지명, 기관명 등 개체명 태그를 붙여주는 과제입니다. 예컨대 다음과 같습니다.

- 그 제품 삼성 건가요 → `그: O`, `제품: O`, `삼성: 기관명`, `건가요: O`

이번 튜토리얼에서 사용할 데이터는 [한국해양대학교 자연언어처리 연구실](https://github.com/kmounlp/NER)에서 공개한 데이터를 사용합니다. 여기에 자체적으로 제작한 데이터를 추가로 포함하였습니다. 자체 제작 데이터는 윤주성 님이 개발한 [BERT 기반 개체명 인식 모델](https://github.com/eagle705/pytorch-bert-crf-ner)로 초벌 레이블링을 수행한 뒤 수작업으로 해당 레이블의 정확도를 검토해 만들었습니다. 그 포맷은 `원본 문장␞레이블링한 문장`이며 그 예시는 다음과 같습니다.

- ―효진 역의 김환희(14)가 특히 인상적이었다.␞―<효진:PER> 역의 <김환희:PER>(<14:NOH>)가 특히 인상적이었다.

이번 튜토리얼에서 사용한 데이터의 개체명 태그 체계는 다음과 같습니다. 총 10개 태그입니다.

|태그|종류|
|---|---|
|PER|인명|
|LOC|지명|
|ORG|기관명|
|DAT|날짜|
|TIM|시간|
|DUR|기간|
|MNY|통화|
|PNT|비율|
|NOH|기타 수량표현|
|POH|기타|


우리가 만들 개체명 인식 모델의 입력은 토큰 시퀀스입니다. 위의 예시를 기준으로 설명한다면 모델의 입력은 [그, 제품, 삼성, 건가요]가 됩니다. 이 모델의 출력은 해당 토큰이 어떤 개체명 태그에 속할지 확률을 나타냅니다. 다음과 같습니다.

- [현재 토큰이 인명(PER)일 확률, 현재 토큰이 지명(LOC)일 확률, 현재 토큰이 기관명(ORG)일 확률, 현재 토큰이 날짜(DAT)일 확률, 현재 토큰이 시간(TIM)일 확률, 현재 토큰이 기간(DUR)일 확률, 현재 토큰이 통화(MNY)일 확률, 현재 토큰이 비율(PNT)일 확률, 현재 토큰이 기타 수량표현(NOH)일 확률, 현재 토큰이 기타(NOH)일 확률, 현재 토큰이 어떤 개체명도 아닐 확률]

개체명 인식 모델은 입력 토큰 각각에 대해 개체명 확률을 반환합니다. 예컨대 입력된 토큰이 5개라면 5개 토큰 각각에 대해 해당 토큰이 어떤 개체명에 속할지 확률값을 내어줍니다. 이를 적당한 후처리(post processing) 과정을 거쳐 사람이 보기에 좋은 형태로 가공해 주면 문장 내에서 어떤 부분이 개체명에 속하는지 알 수 있게 됩니다.


---



## 모델 구조


우리 책에서 사용하는 개체명 인식 모델은 그림1과 같은 구조입니다. 입력 문장을 토큰화한 뒤 문장 시작과 끝을 알리는 스페셜 토큰 `CLS`와 `SEP`를 각각 원래 토큰 시퀀스 앞뒤에 붙입니다. 이를 BERT 모델에 입력하고 모든 토큰에 대해 BERT 모델 마지막 레이어 출력을 뽑습니다. 이들 토큰 벡터 각각에 작은 추가 모듈을 덧붙여 모델의 출력이 '해당 토큰이 특정 개체명에 속할 확률'이 되도록 합니다.

## **그림1** 개체명 인식
{: .no_toc .text-delta }
<img src="https://i.imgur.com/MjbcakT.png" width="350px" title="source: imgur.com" />


---

## 태스크 모듈

개체명 인식 모델에 붙는 태스크 모듈의 구조는 그림2와 같습니다. 우선 마지막 레이어의 개별 토큰 벡터(그림2에서 $\mathbf{x}$)에 드롭아웃(dropout)을 적용합니다. 그 다음 가중치 행렬(weight matrix)을 곱해줘 `CLS`를 분류해야할 범주 수만큼의 차원을 갖는 벡터로 변환합니다(그림 4-2 $\mathbf{x}$에서 $\text{net}$). 만일 `CLS` 벡터가 768차원이고 분류 대상 범주 수가 11개(개체명 태그 10종류 + ' 개체명 아님' 범주)라면 가중치 행렬 크기는 768 $\times$ 11이 됩니다.

## **그림2** 개체명 인식 태스크 모듈 (1)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/tDKNUJh.png" width="350px" title="source: imgur.com" />

이후 여기에 소프트맥스(softmax) 함수를 취해 준 것이 모델의 최종 출력이 됩니다. 소프트맥스 함수는 입력 벡터의 모든 요소(element) 값의 범위를 0\~1로 하고 모든 요소값의 총합을 1이 되게끔 합니다. 다시 말해 어떤 입력 벡터든 소프트맥스 함수를 적용하면 해당 벡터가 확률로 변환된다는 이야기입니다.

다만 개체명 인식 모델은 토큰 각각에 대해 개체명 확률을 내어주기 때문에 실제로는 그림3과 같은 구조가 됩니다. 이렇게 만든 모델의 최종 출력과 정답 레이블을 비교해 모델 출력이 정답 레이블과 최대한 같아지도록 BERT 레이어 전체를 포함한 모델 전체를 업데이트합니다. 이를 학습(train)이라고 합니다. 

## **그림3** 개체명 인식 태스크 모듈 (2)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/U5mPOBh.png" title="source: imgur.com" />


---
